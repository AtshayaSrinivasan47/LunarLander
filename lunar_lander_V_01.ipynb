{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym # pip install gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "# import seaborn as sns\n",
    "# sns.set_theme()\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "from rl.agents import DQNAgent  # pip install keras-rl2\n",
    "from rl.policy import BoltzmannQPolicy  # important to have gym==0.25.2\n",
    "from rl.memory import SequentialMemory\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")  # no render mode to prevent display while training\n",
    "\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n\n",
    "\n",
    "print(states)\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1, states)))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(actions, activation=\"linear\"))\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(\n",
    "    model=model,\n",
    "    memory=SequentialMemory(limit=50000, window_length=1),\n",
    "    policy=BoltzmannQPolicy(),\n",
    "    nb_actions=actions,\n",
    "    nb_steps_warmup=100,\n",
    "    target_model_update=0.01\n",
    ")\n",
    "\n",
    "agent.compile(Adam(lr=0.001), metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom callback to record loss values during training\n",
    "class LossHistory(Callback):\n",
    "    def __init__(self):\n",
    "        self.loss_values = []  # Initialize the loss_values list\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.loss_values.append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the callback\n",
    "loss_history = LossHistory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 176s 18ms/step - reward: -0.4001\n",
      "31 episodes - episode_reward: -131.149 [-440.066, 12.687] - loss: 7.122 - mae: 14.608 - mean_q: 6.122\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 204s 20ms/step - reward: 0.0203\n",
      "11 episodes - episode_reward: 16.944 [-52.993, 167.654] - loss: 4.998 - mae: 26.895 - mean_q: 31.798\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 202s 20ms/step - reward: 0.0937\n",
      "11 episodes - episode_reward: 84.629 [-12.659, 151.024] - loss: 5.366 - mae: 31.091 - mean_q: 40.635\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: 0.0714\n",
      "10 episodes - episode_reward: 71.479 [-48.434, 139.780] - loss: 5.306 - mae: 32.761 - mean_q: 44.217\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: 0.08050s - reward: \n",
      "done, took 1009.290 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the number of training steps\n",
    "total_steps = 50000\n",
    "\n",
    "# Fit the agent to the environment\n",
    "history = agent.fit(\n",
    "    env,\n",
    "    nb_steps=total_steps,\n",
    "    visualize=False,\n",
    "    verbose=1,\n",
    "    callbacks=[loss_history]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect loss and reward values from the callback\n",
    "losses = loss_history.loss_values\n",
    "rewards=history.history['episode_reward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "agent.save_weights('trained_model_weights.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Plot the learning progress\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, 1001), losses, label='Loss')\n",
    "plt.plot(range(1, 1001), rewards, label='Mean Reward')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Learning Progress')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Plot the loss curve\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(loss_history\u001b[38;5;241m.\u001b[39mloss_values)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.plot(loss_history.loss_values)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load_weights(\"trained_model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 281.826, steps: 301\n",
      "Episode 2: reward: 232.623, steps: 241\n",
      "Episode 3: reward: 288.416, steps: 325\n",
      "Episode 4: reward: 253.049, steps: 247\n",
      "Episode 5: reward: 263.854, steps: 340\n",
      "Episode 6: reward: 179.674, steps: 1000\n",
      "Episode 7: reward: 241.297, steps: 330\n",
      "Episode 8: reward: 282.828, steps: 195\n",
      "Episode 9: reward: 253.156, steps: 294\n",
      "Episode 10: reward: 284.089, steps: 258\n"
     ]
    }
   ],
   "source": [
    "results = agent.test(env, nb_episodes=10, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DQNAgent' object has no attribute 'act'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m      5\u001b[0m     frames\u001b[38;5;241m.\u001b[39mappend(env\u001b[38;5;241m.\u001b[39mrender(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m----> 6\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m(state, test\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m     state, _, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Save the video of the episode\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DQNAgent' object has no attribute 'act'"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    frames.append(env.render(mode='rgb_array'))\n",
    "    action = agent.act(state, test=True)\n",
    "    state, _, done, _ = env.step(action)\n",
    "\n",
    "# Save the video of the episode\n",
    "imageio.mimsave(\"episode_video.mp4\", frames, fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
